# Knack RAG Architecture

## Overview

This document outlines the recommended architecture for providing chat and RAG access to Knack.app datasets using knack-sleuth and knack-elt.

## Design Philosophy

**Schema = Static Context** | **Data = Dynamic MCP Tools** | **Freshness = Hybrid Strategy**

- Schema information is relatively static and fits in modern LLM context windows (200k+ tokens)
- Data is dynamic, large, and requires querying infrastructure
- Don't over-engineer: Use simple file reads for schema, MCP only for data operations
- **Data freshness matters:** Use LanceDB for discovery, Knack API for authoritative data (see [DATA_FRESHNESS_STRATEGY.md](DATA_FRESHNESS_STRATEGY.md))

## Architecture Layers

```
┌─────────────────────────────────────────────────┐
│              User Chat Interface                │
└───────────────────┬─────────────────────────────┘
                    │
┌───────────────────▼─────────────────────────────┐
│           LLM Agent (Claude, etc.)              │
│                                                  │
│  Context Window:                                │
│  ├─ .knack/schema.yaml (structure)             │
│  ├─ .knack/relationships.md (ER diagram)       │
│  └─ .knack/schema.md (summary)                 │
└────────┬──────────────────────────┬─────────────┘
         │                          │
         │ (file reads)             │ (MCP calls)
         │                          │
┌────────▼────────┐      ┌──────────▼──────────────┐
│  Static Schema  │      │   MCP Server            │
│  Documentation  │      │   (Data Operations)     │
│                 │      │                         │
│  Generated by:  │      │   Tools:                │
│  knack-sleuth   │      │   - search_records()    │
│                 │      │   - query_records()     │
│                 │      │   - get_record()        │
│                 │      │   - traverse_relation() │
└─────────────────┘      └──────────┬──────────────┘
                                    │
                         ┌──────────▼──────────────┐
                         │      LanceDB            │
                         │                         │
                         │  Tables:                │
                         │  ├─ knack_object_1     │
                         │  ├─ knack_object_2     │
                         │  └─ ...                │
                         │                         │
                         │  Enables:               │
                         │  ├─ Vector search      │
                         │  ├─ SQL queries        │
                         │  └─ Hybrid queries     │
                         └─────────────────────────┘
```

## Data Pipeline

### Phase 1: Schema Export (knack-sleuth)

```bash
# One-time setup or when schema changes
mkdir -p .knack

# Generate human-readable overview
knack-sleuth app-summary --format markdown -o .knack/schema.md

# Generate machine-readable full schema
knack-sleuth export-db-schema --format yaml --detail standard -o .knack/schema.yaml

# Generate ER diagram
knack-sleuth export-db-schema --format mermaid --detail minimal -o .knack/relationships.md

# Generate object catalog
knack-sleuth list-objects > .knack/objects-catalog.txt
```

**Output Example (.knack/schema.yaml):**
```yaml
application:
  name: My Knack App
  slug: my-app
  id: abc123

objects:
  - key: object_1
    name: Customers
    fields:
      - key: field_23
        name: Full Name
        type: name
        required: true
      - key: field_24
        name: Email
        type: email
        required: true
      - key: field_25
        name: Company
        type: connection
        relationship:
          object: object_8
          type: many-to-one
    connections:
      inbound: 3
      outbound: 2
```

### Phase 2: Data Export & Ingestion (knack-elt → LanceDB)

**Key insight:** Store ALL fields as columns, embed SELECTED text fields for semantic search. See [EMBEDDING_STRATEGY.md](EMBEDDING_STRATEGY.md) for details.

```python
from knack_sleuth import load_app_metadata
from knack_elt import export_data  # your library
import lancedb
from sentence_transformers import SentenceTransformer

# Load schema
app = load_app_metadata(app_id="your_app_id")

# Initialize embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Connect to LanceDB
db = lancedb.connect("./lancedb")

# For each object, export data and create LanceDB table
for obj in app.objects:
    print(f"Processing {obj.name} ({obj.key})...")

    # Export data from Knack
    records = export_data(obj.key)

    # Transform for LanceDB
    lance_records = []
    for record in records:
        # Identify text fields for embedding (not numbers, dates, connections)
        text_fields = [
            f for f in obj.fields
            if f.type in ['short_text', 'paragraph_text', 'rich_text', 'email', 'name', 'address']
        ]

        # Concatenate embeddable text content only
        content = " ".join([
            str(record.get(f.key, ""))
            for f in text_fields
        ])

        # Create embedding (only for text content)
        embedding = embedder.encode(content).tolist()

        # Build LanceDB record - preserves ALL original fields
        lance_record = {
            # All original Knack fields as columns (for SQL queries/display)
            **record,

            # Generated fields for semantic search
            "content": content,        # Concatenated text
            "vector": embedding,       # Embedding vector

            # Metadata
            "_object_key": obj.key,
            "_object_name": obj.name,
        }
        lance_records.append(lance_record)

    # Create table
    table_name = f"knack_{obj.key}"
    db.create_table(table_name, lance_records, mode="overwrite")
    print(f"✓ Created table {table_name} with {len(lance_records)} records")
```

### Phase 3: MCP Server (Hybrid Strategy)

**Key Insight:** Use LanceDB for discovery (semantic search), Knack API for fresh data

```python
import lancedb
import httpx
from mcp.server import Server
from mcp.server.stdio import stdio_server
import mcp.types as types

# Initialize LanceDB connection
db = lancedb.connect("./lancedb")

# Initialize Knack API client
knack_client = httpx.AsyncClient(
    base_url="https://api.knack.com/v1",
    headers={"X-Knack-Application-Id": KNACK_APP_ID, "X-Knack-REST-API-Key": KNACK_API_KEY}
)

# Initialize MCP server
app = Server("knack-data-server")

@app.list_tools()
async def list_tools() -> list[types.Tool]:
    return [
        types.Tool(
            name="search_and_fetch",
            description="Semantic search in LanceDB, return fresh data from Knack API. Best for exploratory queries.",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Semantic search query"},
                    "object_key": {"type": "string", "description": "Knack object key (e.g., 'object_1')"},
                    "limit": {"type": "integer", "default": 5}
                },
                "required": ["query", "object_key"]
            }
        ),
        types.Tool(
            name="query_records",
            description="SQL-style query on Knack object data",
            inputSchema={
                "type": "object",
                "properties": {
                    "object_key": {"type": "string"},
                    "where": {"type": "string", "description": "SQL WHERE clause"},
                    "limit": {"type": "integer", "default": 100}
                },
                "required": ["object_key"]
            }
        ),
        types.Tool(
            name="get_record",
            description="Get a specific record by ID",
            inputSchema={
                "type": "object",
                "properties": {
                    "object_key": {"type": "string"},
                    "record_id": {"type": "string"}
                },
                "required": ["object_key", "record_id"]
            }
        ),
    ]

@app.call_tool()
async def call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    if name == "search_and_fetch":
        # Phase 1: Semantic search in LanceDB (find relevant records)
        table = db.open_table(f"knack_{arguments['object_key']}")

        from sentence_transformers import SentenceTransformer
        embedder = SentenceTransformer('all-MiniLM-L6-v2')
        query_embedding = embedder.encode(arguments['query']).tolist()

        # Find similar records
        matches = (
            table.search(query_embedding)
            .limit(arguments.get('limit', 5))
            .to_list()
        )

        # Phase 2: Fetch fresh data from Knack API
        fresh_records = []
        for match in matches:
            response = await knack_client.get(
                f"/objects/{arguments['object_key']}/records/{match['id']}"
            )
            fresh_record = response.json()
            fresh_record['_similarity_score'] = 1 - match.get('_distance', 0)
            fresh_records.append(fresh_record)

        return [types.TextContent(
            type="text",
            text=json.dumps({
                "query": arguments['query'],
                "records": fresh_records,
                "data_source": "hybrid (search: lancedb, data: knack api)",
                "freshness": "real-time"
            }, indent=2)
        )]

    elif name == "query_records":
        table = db.open_table(f"knack_{arguments['object_key']}")

        query = table.search()
        if "where" in arguments:
            query = query.where(arguments["where"])

        results = query.limit(arguments.get('limit', 100)).to_list()

        return [types.TextContent(
            type="text",
            text=json.dumps(results, indent=2)
        )]

    elif name == "get_record":
        table = db.open_table(f"knack_{arguments['object_key']}")

        results = (
            table.search()
            .where(f"id = '{arguments['record_id']}'")
            .to_list()
        )

        if not results:
            return [types.TextContent(type="text", text="Record not found")]

        return [types.TextContent(
            type="text",
            text=json.dumps(results[0], indent=2)
        )]

    raise ValueError(f"Unknown tool: {name}")

# Run server
async def main():
    async with stdio_server() as (read_stream, write_stream):
        await app.run(read_stream, write_stream, app.create_initialization_options())

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

## Usage Examples

### Example 1: Understanding Schema (No MCP)

**User:** "What objects are in my Knack app?"

**Agent Process:**
1. Reads `.knack/schema.yaml` (already in context)
2. Parses YAML to extract object list
3. Responds immediately

**No network calls. No MCP. Just file reads.**

### Example 2: Searching Data (Hybrid MCP)

**User:** "Find all customers from Acme Corporation"

**Agent Process:**
1. Reads `.knack/schema.yaml` to understand Customers object structure
2. Identifies that Customers is `object_1` with a `Company` field
3. Calls MCP tool: `search_and_fetch(query="Acme Corporation", object_key="object_1")`
   - Searches LanceDB for semantic matches
   - Fetches fresh data from Knack API for found records
4. Returns fresh, authoritative data with similarity scores

### Example 3: Complex Query (Hybrid)

**User:** "Show me all high-value orders from the last quarter, and tell me what fields an order has"

**Agent Process:**
1. Schema question: Reads `.knack/schema.yaml` → "Orders has these fields: amount, date, customer..."
2. Data question: Calls `query_records(object_key="object_5", where="amount > 10000 AND date > '2024-10-01'")`
3. Combines both into response

## Benefits of This Approach

| Aspect | Static Schema Files | MCP for Data | Why? |
|--------|-------------------|--------------|------|
| **Schema Queries** | ✅ | ❌ | Schema fits in context, no API needed |
| **Data Queries** | ❌ | ✅ | Data too large for context |
| **Latency** | Instant (file read) | Network call | Schema accessed constantly |
| **Complexity** | Very simple | Moderate | Don't over-engineer |
| **Offline Mode** | ✅ Works | ❌ Needs server | Schema available offline |
| **Version Control** | ✅ Git-friendly | ❌ Not applicable | Schema changes tracked |

## LanceDB Advantages

1. **Hybrid Queries**: Vector search (semantic) + SQL (structured filters)
   ```python
   # Find similar customers (vector) with revenue > $10k (SQL)
   results = table.search(embedding).where("revenue > 10000").to_list()
   ```

2. **No Separate Vector DB**: One database for both vector and tabular data

3. **Column Storage**: Efficient for analytics queries

4. **Versioning**: Built-in data versioning (useful for temporal queries)

5. **Embedded & Serverless**: Can run locally or in cloud

## Data Freshness Strategy

**Critical consideration:** LanceDB is a read replica - it will always lag behind the live Knack database.

### When to Use Each Data Source

| Query Type | Use | Reason | Example |
|------------|-----|--------|---------|
| **Semantic/fuzzy search** | LanceDB → Knack API | Find with vectors, fetch fresh data | "customers who mentioned refunds" |
| **Single record lookup** | Knack API only | Direct fetch faster | "Get customer ID 12345" |
| **Analytics/aggregations** | LanceDB only | Staleness acceptable | "Average revenue by region" |
| **Write operations** | Knack API only | Only source of truth can write | "Update customer email" |
| **Financial/compliance** | Knack API only | Zero staleness tolerance | "Current account balance" |

### Recommended Hybrid Pattern

**Best practice:** Search in LanceDB (for discovery), fetch from Knack API (for fresh data)

```python
# User asks: "Find customers with billing issues"

# Step 1: Semantic search in LanceDB (uses embeddings)
matches = lancedb.search(embed("billing issues"))  # Returns IDs

# Step 2: Fetch fresh data from Knack API
for match in matches:
    fresh_data = knack_api.get_record(match['id'])  # Real-time data
```

**Why this works:**
- ✅ Leverage semantic search (impossible on Knack API)
- ✅ Always return authoritative, fresh data
- ✅ Staleness only affects which records are found, not the data shown

### Sync Strategies

Choose based on your needs:

| Strategy | Latency | Complexity | Best For |
|----------|---------|------------|----------|
| **Nightly full refresh** | 24h | Low | Small datasets, infrequent changes |
| **Incremental (15min)** | 15min | Medium | Medium datasets, frequent updates |
| **Webhook-triggered** | Seconds | High | Need near real-time search |
| **On-demand refresh** | Variable | Low | User-initiated refresh |

**See [DATA_FRESHNESS_STRATEGY.md](DATA_FRESHNESS_STRATEGY.md) for detailed implementation guidance.**

## Maintenance Workflow

### When Schema Changes

```bash
# Re-export schema (takes ~5 seconds)
knack-sleuth app-summary --format markdown -o .knack/schema.md
knack-sleuth export-db-schema --format yaml -o .knack/schema.yaml

# Agent automatically picks up changes (files in context)
```

### When Data Changes

```bash
# Re-run data sync (Python script)
python sync_knack_to_lancedb.py

# MCP server automatically uses updated LanceDB tables
```

## Alternative: When to Use MCP for Schema

**Use MCP for schema if:**
- Schema is massive (>50k tokens) and won't fit in context
- Need real-time schema changes (unlikely for Knack)
- Want vector search over schema itself (e.g., "find objects related to payments")

**In this case, add ONE more MCP tool:**
```python
@app.tool()
async def search_schema(query: str) -> dict:
    """Semantic search over schema metadata"""
    # Search knack_schemas table in LanceDB
    ...
```

But for most Knack apps, static files are simpler and faster.

## Next Steps

1. ✅ Use knack-sleuth to export schema
2. ⏳ Build knack-elt for data export
3. ⏳ Create LanceDB ingestion script
4. ⏳ Implement MCP server for data operations
5. ⏳ Test with Claude Desktop or other MCP clients

## Tools Required

- **knack-sleuth**: Already built (schema export)
- **knack-elt**: To build (data export from Knack API)
- **LanceDB**: `pip install lancedb`
- **MCP SDK**: `pip install mcp`
- **Embedding Model**: `pip install sentence-transformers`

## Estimated Complexity

| Component | Complexity | Time Estimate |
|-----------|-----------|---------------|
| Schema export setup | Very Low | 10 minutes |
| knack-elt library | Medium | 2-4 hours |
| LanceDB ingestion | Low | 1-2 hours |
| MCP server | Low-Medium | 2-3 hours |
| **Total** | **Medium** | **~1 day** |

Much simpler than building MCP tools for schema!
