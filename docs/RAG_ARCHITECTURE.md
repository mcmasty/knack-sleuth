# Knack RAG Architecture

## Overview

This document outlines the recommended architecture for providing chat and RAG access to Knack.app datasets using knack-sleuth and knack-elt.

## Design Philosophy

**Schema = Static Context** | **Data = Dynamic MCP Tools**

- Schema information is relatively static and fits in modern LLM context windows (200k+ tokens)
- Data is dynamic, large, and requires querying infrastructure
- Don't over-engineer: Use simple file reads for schema, MCP only for data operations

## Architecture Layers

```
┌─────────────────────────────────────────────────┐
│              User Chat Interface                │
└───────────────────┬─────────────────────────────┘
                    │
┌───────────────────▼─────────────────────────────┐
│           LLM Agent (Claude, etc.)              │
│                                                  │
│  Context Window:                                │
│  ├─ .knack/schema.yaml (structure)             │
│  ├─ .knack/relationships.md (ER diagram)       │
│  └─ .knack/schema.md (summary)                 │
└────────┬──────────────────────────┬─────────────┘
         │                          │
         │ (file reads)             │ (MCP calls)
         │                          │
┌────────▼────────┐      ┌──────────▼──────────────┐
│  Static Schema  │      │   MCP Server            │
│  Documentation  │      │   (Data Operations)     │
│                 │      │                         │
│  Generated by:  │      │   Tools:                │
│  knack-sleuth   │      │   - search_records()    │
│                 │      │   - query_records()     │
│                 │      │   - get_record()        │
│                 │      │   - traverse_relation() │
└─────────────────┘      └──────────┬──────────────┘
                                    │
                         ┌──────────▼──────────────┐
                         │      LanceDB            │
                         │                         │
                         │  Tables:                │
                         │  ├─ knack_object_1     │
                         │  ├─ knack_object_2     │
                         │  └─ ...                │
                         │                         │
                         │  Enables:               │
                         │  ├─ Vector search      │
                         │  ├─ SQL queries        │
                         │  └─ Hybrid queries     │
                         └─────────────────────────┘
```

## Data Pipeline

### Phase 1: Schema Export (knack-sleuth)

```bash
# One-time setup or when schema changes
mkdir -p .knack

# Generate human-readable overview
knack-sleuth app-summary --format markdown -o .knack/schema.md

# Generate machine-readable full schema
knack-sleuth export-db-schema --format yaml --detail standard -o .knack/schema.yaml

# Generate ER diagram
knack-sleuth export-db-schema --format mermaid --detail minimal -o .knack/relationships.md

# Generate object catalog
knack-sleuth list-objects > .knack/objects-catalog.txt
```

**Output Example (.knack/schema.yaml):**
```yaml
application:
  name: My Knack App
  slug: my-app
  id: abc123

objects:
  - key: object_1
    name: Customers
    fields:
      - key: field_23
        name: Full Name
        type: name
        required: true
      - key: field_24
        name: Email
        type: email
        required: true
      - key: field_25
        name: Company
        type: connection
        relationship:
          object: object_8
          type: many-to-one
    connections:
      inbound: 3
      outbound: 2
```

### Phase 2: Data Export & Ingestion (knack-elt → LanceDB)

```python
from knack_sleuth import load_app_metadata
from knack_elt import export_data  # your library
import lancedb
from sentence_transformers import SentenceTransformer

# Load schema
app = load_app_metadata(app_id="your_app_id")

# Initialize embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Connect to LanceDB
db = lancedb.connect("./lancedb")

# For each object, export data and create LanceDB table
for obj in app.objects:
    print(f"Processing {obj.name} ({obj.key})...")

    # Export data from Knack
    records = export_data(obj.key)

    # Transform for LanceDB
    lance_records = []
    for record in records:
        # Identify text fields for embedding
        text_fields = [
            f for f in obj.fields
            if f.type in ['short_text', 'paragraph_text', 'rich_text', 'email']
        ]

        # Concatenate text content
        content = " ".join([
            str(record.get(f.key, ""))
            for f in text_fields
        ])

        # Create embedding
        embedding = embedder.encode(content).tolist()

        # Build LanceDB record
        lance_record = {
            "id": record["id"],
            "content": content,
            "vector": embedding,
            "_object_key": obj.key,
            "_object_name": obj.name,
            **record  # Include all original fields for SQL queries
        }
        lance_records.append(lance_record)

    # Create table
    table_name = f"knack_{obj.key}"
    db.create_table(table_name, lance_records, mode="overwrite")
    print(f"✓ Created table {table_name} with {len(lance_records)} records")
```

### Phase 3: MCP Server (Data Operations Only)

```python
import lancedb
from mcp.server import Server
from mcp.server.stdio import stdio_server
import mcp.types as types

# Initialize LanceDB connection
db = lancedb.connect("./lancedb")

# Initialize MCP server
app = Server("knack-data-server")

@app.list_tools()
async def list_tools() -> list[types.Tool]:
    return [
        types.Tool(
            name="search_records",
            description="Semantic search across records in a Knack object",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "object_key": {"type": "string", "description": "Knack object key (e.g., 'object_1')"},
                    "limit": {"type": "integer", "default": 10}
                },
                "required": ["query", "object_key"]
            }
        ),
        types.Tool(
            name="query_records",
            description="SQL-style query on Knack object data",
            inputSchema={
                "type": "object",
                "properties": {
                    "object_key": {"type": "string"},
                    "where": {"type": "string", "description": "SQL WHERE clause"},
                    "limit": {"type": "integer", "default": 100}
                },
                "required": ["object_key"]
            }
        ),
        types.Tool(
            name="get_record",
            description="Get a specific record by ID",
            inputSchema={
                "type": "object",
                "properties": {
                    "object_key": {"type": "string"},
                    "record_id": {"type": "string"}
                },
                "required": ["object_key", "record_id"]
            }
        ),
    ]

@app.call_tool()
async def call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    if name == "search_records":
        table = db.open_table(f"knack_{arguments['object_key']}")

        # Generate embedding for query
        from sentence_transformers import SentenceTransformer
        embedder = SentenceTransformer('all-MiniLM-L6-v2')
        query_embedding = embedder.encode(arguments['query']).tolist()

        # Vector search
        results = (
            table.search(query_embedding)
            .limit(arguments.get('limit', 10))
            .to_list()
        )

        return [types.TextContent(
            type="text",
            text=json.dumps(results, indent=2)
        )]

    elif name == "query_records":
        table = db.open_table(f"knack_{arguments['object_key']}")

        query = table.search()
        if "where" in arguments:
            query = query.where(arguments["where"])

        results = query.limit(arguments.get('limit', 100)).to_list()

        return [types.TextContent(
            type="text",
            text=json.dumps(results, indent=2)
        )]

    elif name == "get_record":
        table = db.open_table(f"knack_{arguments['object_key']}")

        results = (
            table.search()
            .where(f"id = '{arguments['record_id']}'")
            .to_list()
        )

        if not results:
            return [types.TextContent(type="text", text="Record not found")]

        return [types.TextContent(
            type="text",
            text=json.dumps(results[0], indent=2)
        )]

    raise ValueError(f"Unknown tool: {name}")

# Run server
async def main():
    async with stdio_server() as (read_stream, write_stream):
        await app.run(read_stream, write_stream, app.create_initialization_options())

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

## Usage Examples

### Example 1: Understanding Schema (No MCP)

**User:** "What objects are in my Knack app?"

**Agent Process:**
1. Reads `.knack/schema.yaml` (already in context)
2. Parses YAML to extract object list
3. Responds immediately

**No network calls. No MCP. Just file reads.**

### Example 2: Searching Data (Uses MCP)

**User:** "Find all customers from Acme Corporation"

**Agent Process:**
1. Reads `.knack/schema.yaml` to understand Customers object structure
2. Identifies that Customers is `object_1` with a `Company` field
3. Calls MCP tool: `search_records(query="Acme Corporation", object_key="object_1")`
4. Returns results from LanceDB

### Example 3: Complex Query (Hybrid)

**User:** "Show me all high-value orders from the last quarter, and tell me what fields an order has"

**Agent Process:**
1. Schema question: Reads `.knack/schema.yaml` → "Orders has these fields: amount, date, customer..."
2. Data question: Calls `query_records(object_key="object_5", where="amount > 10000 AND date > '2024-10-01'")`
3. Combines both into response

## Benefits of This Approach

| Aspect | Static Schema Files | MCP for Data | Why? |
|--------|-------------------|--------------|------|
| **Schema Queries** | ✅ | ❌ | Schema fits in context, no API needed |
| **Data Queries** | ❌ | ✅ | Data too large for context |
| **Latency** | Instant (file read) | Network call | Schema accessed constantly |
| **Complexity** | Very simple | Moderate | Don't over-engineer |
| **Offline Mode** | ✅ Works | ❌ Needs server | Schema available offline |
| **Version Control** | ✅ Git-friendly | ❌ Not applicable | Schema changes tracked |

## LanceDB Advantages

1. **Hybrid Queries**: Vector search (semantic) + SQL (structured filters)
   ```python
   # Find similar customers (vector) with revenue > $10k (SQL)
   results = table.search(embedding).where("revenue > 10000").to_list()
   ```

2. **No Separate Vector DB**: One database for both vector and tabular data

3. **Column Storage**: Efficient for analytics queries

4. **Versioning**: Built-in data versioning (useful for temporal queries)

5. **Embedded & Serverless**: Can run locally or in cloud

## Maintenance Workflow

### When Schema Changes

```bash
# Re-export schema (takes ~5 seconds)
knack-sleuth app-summary --format markdown -o .knack/schema.md
knack-sleuth export-db-schema --format yaml -o .knack/schema.yaml

# Agent automatically picks up changes (files in context)
```

### When Data Changes

```bash
# Re-run data sync (Python script)
python sync_knack_to_lancedb.py

# MCP server automatically uses updated LanceDB tables
```

## Alternative: When to Use MCP for Schema

**Use MCP for schema if:**
- Schema is massive (>50k tokens) and won't fit in context
- Need real-time schema changes (unlikely for Knack)
- Want vector search over schema itself (e.g., "find objects related to payments")

**In this case, add ONE more MCP tool:**
```python
@app.tool()
async def search_schema(query: str) -> dict:
    """Semantic search over schema metadata"""
    # Search knack_schemas table in LanceDB
    ...
```

But for most Knack apps, static files are simpler and faster.

## Next Steps

1. ✅ Use knack-sleuth to export schema
2. ⏳ Build knack-elt for data export
3. ⏳ Create LanceDB ingestion script
4. ⏳ Implement MCP server for data operations
5. ⏳ Test with Claude Desktop or other MCP clients

## Tools Required

- **knack-sleuth**: Already built (schema export)
- **knack-elt**: To build (data export from Knack API)
- **LanceDB**: `pip install lancedb`
- **MCP SDK**: `pip install mcp`
- **Embedding Model**: `pip install sentence-transformers`

## Estimated Complexity

| Component | Complexity | Time Estimate |
|-----------|-----------|---------------|
| Schema export setup | Very Low | 10 minutes |
| knack-elt library | Medium | 2-4 hours |
| LanceDB ingestion | Low | 1-2 hours |
| MCP server | Low-Medium | 2-3 hours |
| **Total** | **Medium** | **~1 day** |

Much simpler than building MCP tools for schema!
